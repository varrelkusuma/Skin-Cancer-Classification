{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(0)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/ravee/Jupyter/Skin-Cancer-Classification/data/data3.csv', header=None)\n",
    "X = dataset.iloc[:, 0:61].values\n",
    "Y = dataset.iloc[:, 62].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.374839</td>\n",
       "      <td>63.340830</td>\n",
       "      <td>112.263065</td>\n",
       "      <td>55.737793</td>\n",
       "      <td>52.463312</td>\n",
       "      <td>91.327983</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.032123</td>\n",
       "      <td>1.950237</td>\n",
       "      <td>1.981436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265834</td>\n",
       "      <td>0.995359</td>\n",
       "      <td>0.993634</td>\n",
       "      <td>0.995237</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.076647</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.076181</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.300242</td>\n",
       "      <td>82.668481</td>\n",
       "      <td>137.457367</td>\n",
       "      <td>49.635884</td>\n",
       "      <td>75.253828</td>\n",
       "      <td>121.745271</td>\n",
       "      <td>0.134248</td>\n",
       "      <td>-0.066341</td>\n",
       "      <td>2.015002</td>\n",
       "      <td>1.921611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273114</td>\n",
       "      <td>0.992255</td>\n",
       "      <td>0.989582</td>\n",
       "      <td>0.991906</td>\n",
       "      <td>0.989182</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>0.075512</td>\n",
       "      <td>0.081259</td>\n",
       "      <td>0.074591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.645767</td>\n",
       "      <td>92.508307</td>\n",
       "      <td>148.996292</td>\n",
       "      <td>57.005206</td>\n",
       "      <td>67.924656</td>\n",
       "      <td>106.590925</td>\n",
       "      <td>0.057193</td>\n",
       "      <td>-0.050644</td>\n",
       "      <td>2.050657</td>\n",
       "      <td>2.006676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301113</td>\n",
       "      <td>0.994811</td>\n",
       "      <td>0.991801</td>\n",
       "      <td>0.993681</td>\n",
       "      <td>0.992398</td>\n",
       "      <td>0.099094</td>\n",
       "      <td>0.089396</td>\n",
       "      <td>0.095389</td>\n",
       "      <td>0.090669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.615295</td>\n",
       "      <td>17.457194</td>\n",
       "      <td>27.802821</td>\n",
       "      <td>33.447148</td>\n",
       "      <td>48.877430</td>\n",
       "      <td>76.538757</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>-0.670850</td>\n",
       "      <td>2.222315</td>\n",
       "      <td>2.906180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423064</td>\n",
       "      <td>0.987305</td>\n",
       "      <td>0.982757</td>\n",
       "      <td>0.987311</td>\n",
       "      <td>0.983433</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.178278</td>\n",
       "      <td>0.187297</td>\n",
       "      <td>0.178983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.209791</td>\n",
       "      <td>62.129607</td>\n",
       "      <td>101.965131</td>\n",
       "      <td>57.775117</td>\n",
       "      <td>75.554092</td>\n",
       "      <td>121.504623</td>\n",
       "      <td>0.146505</td>\n",
       "      <td>0.023257</td>\n",
       "      <td>1.958404</td>\n",
       "      <td>2.095389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304045</td>\n",
       "      <td>0.977174</td>\n",
       "      <td>0.968355</td>\n",
       "      <td>0.978547</td>\n",
       "      <td>0.970506</td>\n",
       "      <td>0.100605</td>\n",
       "      <td>0.090623</td>\n",
       "      <td>0.102176</td>\n",
       "      <td>0.092443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>82.044136</td>\n",
       "      <td>83.768722</td>\n",
       "      <td>120.307772</td>\n",
       "      <td>89.496293</td>\n",
       "      <td>89.498528</td>\n",
       "      <td>126.176035</td>\n",
       "      <td>-0.343964</td>\n",
       "      <td>0.078492</td>\n",
       "      <td>2.098720</td>\n",
       "      <td>2.102761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307004</td>\n",
       "      <td>0.986611</td>\n",
       "      <td>0.981444</td>\n",
       "      <td>0.985685</td>\n",
       "      <td>0.981658</td>\n",
       "      <td>0.103888</td>\n",
       "      <td>0.093887</td>\n",
       "      <td>0.101714</td>\n",
       "      <td>0.094251</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>83.793886</td>\n",
       "      <td>79.859312</td>\n",
       "      <td>125.879575</td>\n",
       "      <td>71.660835</td>\n",
       "      <td>68.321651</td>\n",
       "      <td>105.753942</td>\n",
       "      <td>-0.102729</td>\n",
       "      <td>0.081119</td>\n",
       "      <td>1.886748</td>\n",
       "      <td>2.044726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259094</td>\n",
       "      <td>0.985333</td>\n",
       "      <td>0.981057</td>\n",
       "      <td>0.985738</td>\n",
       "      <td>0.980027</td>\n",
       "      <td>0.075938</td>\n",
       "      <td>0.068317</td>\n",
       "      <td>0.076629</td>\n",
       "      <td>0.067130</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>82.216087</td>\n",
       "      <td>63.332681</td>\n",
       "      <td>107.854577</td>\n",
       "      <td>81.626920</td>\n",
       "      <td>64.118188</td>\n",
       "      <td>105.661451</td>\n",
       "      <td>0.564203</td>\n",
       "      <td>-0.436504</td>\n",
       "      <td>2.575104</td>\n",
       "      <td>2.013750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240843</td>\n",
       "      <td>0.990387</td>\n",
       "      <td>0.987980</td>\n",
       "      <td>0.990964</td>\n",
       "      <td>0.987218</td>\n",
       "      <td>0.064741</td>\n",
       "      <td>0.059560</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.058005</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>58.616472</td>\n",
       "      <td>58.697274</td>\n",
       "      <td>100.933170</td>\n",
       "      <td>65.831264</td>\n",
       "      <td>65.248561</td>\n",
       "      <td>108.459665</td>\n",
       "      <td>0.098212</td>\n",
       "      <td>-0.091045</td>\n",
       "      <td>1.973688</td>\n",
       "      <td>1.955592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216076</td>\n",
       "      <td>0.987113</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.986366</td>\n",
       "      <td>0.981405</td>\n",
       "      <td>0.054408</td>\n",
       "      <td>0.048588</td>\n",
       "      <td>0.053877</td>\n",
       "      <td>0.046689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>52.713607</td>\n",
       "      <td>51.453149</td>\n",
       "      <td>63.965753</td>\n",
       "      <td>91.518674</td>\n",
       "      <td>88.980855</td>\n",
       "      <td>109.625390</td>\n",
       "      <td>-0.345323</td>\n",
       "      <td>-0.031681</td>\n",
       "      <td>2.127617</td>\n",
       "      <td>1.802747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469022</td>\n",
       "      <td>0.957642</td>\n",
       "      <td>0.939005</td>\n",
       "      <td>0.951739</td>\n",
       "      <td>0.939479</td>\n",
       "      <td>0.240873</td>\n",
       "      <td>0.220280</td>\n",
       "      <td>0.233988</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1           2          3          4           5   \\\n",
       "0    67.374839  63.340830  112.263065  55.737793  52.463312   91.327983   \n",
       "1    52.300242  82.668481  137.457367  49.635884  75.253828  121.745271   \n",
       "2    76.645767  92.508307  148.996292  57.005206  67.924656  106.590925   \n",
       "3    11.615295  17.457194   27.802821  33.447148  48.877430   76.538757   \n",
       "4    46.209791  62.129607  101.965131  57.775117  75.554092  121.504623   \n",
       "..         ...        ...         ...        ...        ...         ...   \n",
       "916  82.044136  83.768722  120.307772  89.496293  89.498528  126.176035   \n",
       "917  83.793886  79.859312  125.879575  71.660835  68.321651  105.753942   \n",
       "918  82.216087  63.332681  107.854577  81.626920  64.118188  105.661451   \n",
       "919  58.616472  58.697274  100.933170  65.831264  65.248561  108.459665   \n",
       "920  52.713607  51.453149   63.965753  91.518674  88.980855  109.625390   \n",
       "\n",
       "           6         7         8         9   ...        53        54  \\\n",
       "0   -0.033850 -0.032123  1.950237  1.981436  ...  0.265834  0.995359   \n",
       "1    0.134248 -0.066341  2.015002  1.921611  ...  0.273114  0.992255   \n",
       "2    0.057193 -0.050644  2.050657  2.006676  ...  0.301113  0.994811   \n",
       "3    0.077760 -0.670850  2.222315  2.906180  ...  0.423064  0.987305   \n",
       "4    0.146505  0.023257  1.958404  2.095389  ...  0.304045  0.977174   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "916 -0.343964  0.078492  2.098720  2.102761  ...  0.307004  0.986611   \n",
       "917 -0.102729  0.081119  1.886748  2.044726  ...  0.259094  0.985333   \n",
       "918  0.564203 -0.436504  2.575104  2.013750  ...  0.240843  0.990387   \n",
       "919  0.098212 -0.091045  1.973688  1.955592  ...  0.216076  0.987113   \n",
       "920 -0.345323 -0.031681  2.127617  1.802747  ...  0.469022  0.957642   \n",
       "\n",
       "           55        56        57        58        59        60        61  62  \n",
       "0    0.993634  0.995237  0.993613  0.076647  0.070833  0.076181  0.070668   1  \n",
       "1    0.989582  0.991906  0.989182  0.082170  0.075512  0.081259  0.074591   1  \n",
       "2    0.991801  0.993681  0.992398  0.099094  0.089396  0.095389  0.090669   1  \n",
       "3    0.982757  0.987311  0.983433  0.187850  0.178278  0.187297  0.178983   1  \n",
       "4    0.968355  0.978547  0.970506  0.100605  0.090623  0.102176  0.092443   1  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  ..  \n",
       "916  0.981444  0.985685  0.981658  0.103888  0.093887  0.101714  0.094251   3  \n",
       "917  0.981057  0.985738  0.980027  0.075938  0.068317  0.076629  0.067130   3  \n",
       "918  0.987980  0.990964  0.987218  0.064741  0.059560  0.066223  0.058005   3  \n",
       "919  0.982953  0.986366  0.981405  0.054408  0.048588  0.053877  0.046689   3  \n",
       "920  0.939005  0.951739  0.939479  0.240873  0.220280  0.233988  0.219982   3  \n",
       "\n",
       "[921 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important information about the data used in this model\n",
    "# 0 - 11    : Color attributes (Mean, Std Dev)\n",
    "# 12 - 37   : LBP attributes (Texture #1)\n",
    "# 38 - 61  : GLCM attributes (Texture #2)\n",
    "\n",
    "# 62 : Identifier\n",
    "\n",
    "# 1 = Melanoma\n",
    "# 2 = Basal Cell Carcinoma\n",
    "# 3 = Squamous Cell Carcinoma\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.34419918, -1.95484002, -1.48497517, -2.6656988 , -1.96092465,\n",
       "       -1.45043885, -0.26154909, -0.69429462,  0.33026858,  0.24021607,\n",
       "        0.798922  ,  0.90585091, -1.33827607, -1.39843814, -1.51733163,\n",
       "       -0.9415383 , -0.23172429,  0.45497378,  1.25208962,  1.8001798 ,\n",
       "        2.50289295,  2.61697842,  2.78274551,  2.47232673,  2.09322706,\n",
       "        1.39510846,  1.69710425,  1.45551505,  1.44648114,  0.82861778,\n",
       "        1.30258822,  0.70867792,  0.63649836, -0.46630852, -1.02071653,\n",
       "       -1.43666705, -0.45572584, -1.59858272, -0.78971024, -0.67342411,\n",
       "       -0.70137639, -0.70021868, -0.92742581, -0.80691207, -0.80483084,\n",
       "       -0.85757385,  0.95660696,  0.83757261,  0.82595463,  0.89375047,\n",
       "       -1.01680535, -0.97204584, -1.04181874, -0.95578465,  1.1899139 ,\n",
       "        1.17815021,  1.16725874,  1.1887328 , -0.92976239, -0.89442221,\n",
       "       -0.94835319])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation of Naive Bayes\n",
    "# Fitting Naive Bayes to the training set\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64,  4, 11],\n",
       "       [26, 22, 12],\n",
       "       [21, 10, 15]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573435764531655\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = nb, X = X_train, y = y_train, cv = 10)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.80      0.70        40\n",
      "           2       0.54      0.22      0.31        32\n",
      "           3       0.43      0.57      0.49        21\n",
      "\n",
      "    accuracy                           0.55        93\n",
      "   macro avg       0.53      0.53      0.50        93\n",
      "weighted avg       0.55      0.55      0.52        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.87      0.76        39\n",
      "           2       0.65      0.45      0.54        33\n",
      "           3       0.33      0.30      0.32        20\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.55      0.54      0.54        92\n",
      "weighted avg       0.59      0.60      0.58        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.77      0.71        39\n",
      "           2       0.64      0.42      0.51        33\n",
      "           3       0.48      0.60      0.53        20\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.59      0.60      0.59        92\n",
      "weighted avg       0.62      0.61      0.60        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.92      0.78        39\n",
      "           2       0.60      0.36      0.45        33\n",
      "           3       0.42      0.40      0.41        20\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.57      0.56      0.55        92\n",
      "weighted avg       0.59      0.61      0.58        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.67      0.68        39\n",
      "           2       0.65      0.62      0.63        32\n",
      "           3       0.46      0.52      0.49        21\n",
      "\n",
      "    accuracy                           0.62        92\n",
      "   macro avg       0.60      0.61      0.60        92\n",
      "weighted avg       0.63      0.62      0.62        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.67      0.67        39\n",
      "           2       0.48      0.34      0.40        32\n",
      "           3       0.20      0.29      0.24        21\n",
      "\n",
      "    accuracy                           0.47        92\n",
      "   macro avg       0.45      0.43      0.43        92\n",
      "weighted avg       0.49      0.47      0.48        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.85      0.71        39\n",
      "           2       0.64      0.28      0.39        32\n",
      "           3       0.38      0.43      0.40        21\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.54      0.52      0.50        92\n",
      "weighted avg       0.57      0.55      0.53        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.82      0.72        39\n",
      "           2       0.55      0.34      0.42        32\n",
      "           3       0.23      0.24      0.23        21\n",
      "\n",
      "    accuracy                           0.52        92\n",
      "   macro avg       0.47      0.47      0.46        92\n",
      "weighted avg       0.51      0.52      0.51        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.82      0.76        39\n",
      "           2       0.65      0.41      0.50        32\n",
      "           3       0.33      0.43      0.38        21\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.56      0.55      0.55        92\n",
      "weighted avg       0.60      0.59      0.58        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.74      0.67        39\n",
      "           2       0.72      0.41      0.52        32\n",
      "           3       0.44      0.57      0.50        21\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.59      0.57      0.56        92\n",
      "weighted avg       0.61      0.59      0.58        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = nb, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.66      0.79      0.72       391\n",
      "         bcc       0.61      0.39      0.47       323\n",
      "         scc       0.37      0.43      0.40       207\n",
      "\n",
      "    accuracy                           0.57       921\n",
      "   macro avg       0.55      0.54      0.53       921\n",
      "weighted avg       0.58      0.57      0.56       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=25, p=3,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 25, metric = 'minkowski', p = 3, weights = 'distance')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72,  6,  1],\n",
       "       [18, 34,  8],\n",
       "       [16, 18, 12]], dtype=int64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779711218067381\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = knn, X = X_train, y = y_train, cv = 10)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "{'metric': 'minkowski', 'n_neighbors': 25, 'p': 3, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to Find the Best Hyperparameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "    {'n_neighbors': [21, 23, 25, 27, 29], 'weights': ['uniform', 'distance'], 'p': [1, 2, 3], 'metric': ['minkowski']},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = knn, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(round(best_accuracy, 2))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.90      0.81        40\n",
      "           2       0.56      0.56      0.56        32\n",
      "           3       0.58      0.33      0.42        21\n",
      "\n",
      "    accuracy                           0.66        93\n",
      "   macro avg       0.63      0.60      0.60        93\n",
      "weighted avg       0.64      0.66      0.64        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.92      0.81        39\n",
      "           2       0.83      0.76      0.79        33\n",
      "           3       0.67      0.40      0.50        20\n",
      "\n",
      "    accuracy                           0.75        92\n",
      "   macro avg       0.74      0.69      0.70        92\n",
      "weighted avg       0.75      0.75      0.74        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.87      0.83        39\n",
      "           2       0.71      0.73      0.72        33\n",
      "           3       0.53      0.40      0.46        20\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.68      0.67      0.67        92\n",
      "weighted avg       0.70      0.72      0.71        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.92      0.80        39\n",
      "           2       0.63      0.67      0.65        33\n",
      "           3       0.67      0.20      0.31        20\n",
      "\n",
      "    accuracy                           0.67        92\n",
      "   macro avg       0.67      0.60      0.58        92\n",
      "weighted avg       0.67      0.67      0.64        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.69      0.73        39\n",
      "           2       0.63      0.81      0.71        32\n",
      "           3       0.50      0.38      0.43        21\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.64      0.63      0.62        92\n",
      "weighted avg       0.66      0.66      0.66        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.90      0.82        39\n",
      "           2       0.70      0.72      0.71        32\n",
      "           3       0.69      0.43      0.53        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.72      0.68      0.69        92\n",
      "weighted avg       0.72      0.73      0.72        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.97      0.79        39\n",
      "           2       0.68      0.47      0.56        32\n",
      "           3       0.46      0.29      0.35        21\n",
      "\n",
      "    accuracy                           0.64        92\n",
      "   macro avg       0.60      0.58      0.57        92\n",
      "weighted avg       0.63      0.64      0.61        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.87      0.80        39\n",
      "           2       0.68      0.59      0.63        32\n",
      "           3       0.61      0.52      0.56        21\n",
      "\n",
      "    accuracy                           0.70        92\n",
      "   macro avg       0.68      0.66      0.67        92\n",
      "weighted avg       0.69      0.70      0.69        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.85      0.85        39\n",
      "           2       0.64      0.84      0.73        32\n",
      "           3       0.64      0.33      0.44        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.71      0.67      0.67        92\n",
      "weighted avg       0.73      0.73      0.71        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.82      0.76        39\n",
      "           2       0.55      0.56      0.55        32\n",
      "           3       0.36      0.24      0.29        21\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.54      0.54      0.53        92\n",
      "weighted avg       0.57      0.60      0.58        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = knn, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.74      0.87      0.80       391\n",
      "         bcc       0.66      0.67      0.66       323\n",
      "         scc       0.56      0.35      0.43       207\n",
      "\n",
      "    accuracy                           0.69       921\n",
      "   macro avg       0.65      0.63      0.63       921\n",
      "weighted avg       0.67      0.69      0.67       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation of Support Vector Machine (SVM)\n",
    "# Fitting SVM to the training set\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(C = 100, kernel = 'rbf', gamma = 0.001, random_state = 0)\n",
    "#classifier = SVC(C = 1, kernel = 'poly', degree = 30, random_state = 0)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75,  2,  2],\n",
       "       [ 4, 49,  7],\n",
       "       [ 6, 20, 20]], dtype=int64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7580525731210662\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = svm, X = X_train, y = y_train, cv = 10)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to Find the Best Hyperparameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "    {'C': [1, 10, 100], 'kernel': ['rbf', 'sigmoid', 'poly'], 'gamma': [0.001, 0.01, 0.1, 10]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(round(best_accuracy, 2))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.95      0.90        40\n",
      "           2       0.67      0.69      0.68        32\n",
      "           3       0.50      0.38      0.43        21\n",
      "\n",
      "    accuracy                           0.73        93\n",
      "   macro avg       0.68      0.67      0.67        93\n",
      "weighted avg       0.71      0.73      0.72        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.97      0.96        39\n",
      "           2       0.80      0.85      0.82        33\n",
      "           3       0.71      0.60      0.65        20\n",
      "\n",
      "    accuracy                           0.85        92\n",
      "   macro avg       0.82      0.81      0.81        92\n",
      "weighted avg       0.84      0.85      0.84        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.92      0.94        39\n",
      "           2       0.74      0.85      0.79        33\n",
      "           3       0.75      0.60      0.67        20\n",
      "\n",
      "    accuracy                           0.83        92\n",
      "   macro avg       0.81      0.79      0.80        92\n",
      "weighted avg       0.83      0.83      0.82        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.92      0.90        39\n",
      "           2       0.68      0.70      0.69        33\n",
      "           3       0.53      0.45      0.49        20\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.69      0.69      0.69        92\n",
      "weighted avg       0.73      0.74      0.73        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.85      0.89        39\n",
      "           2       0.65      0.75      0.70        32\n",
      "           3       0.50      0.48      0.49        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.70      0.69      0.69        92\n",
      "weighted avg       0.74      0.73      0.73        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.92      0.91        39\n",
      "           2       0.77      0.75      0.76        32\n",
      "           3       0.57      0.57      0.57        21\n",
      "\n",
      "    accuracy                           0.78        92\n",
      "   macro avg       0.75      0.75      0.75        92\n",
      "weighted avg       0.78      0.78      0.78        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.95      0.89        39\n",
      "           2       0.69      0.69      0.69        32\n",
      "           3       0.44      0.33      0.38        21\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.66      0.66      0.65        92\n",
      "weighted avg       0.70      0.72      0.70        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.97      0.95        39\n",
      "           2       0.76      0.78      0.77        32\n",
      "           3       0.67      0.57      0.62        21\n",
      "\n",
      "    accuracy                           0.82        92\n",
      "   macro avg       0.78      0.78      0.78        92\n",
      "weighted avg       0.81      0.82      0.81        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.95      0.92        39\n",
      "           2       0.82      0.84      0.83        32\n",
      "           3       0.78      0.67      0.72        21\n",
      "\n",
      "    accuracy                           0.85        92\n",
      "   macro avg       0.83      0.82      0.82        92\n",
      "weighted avg       0.84      0.85      0.84        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91        39\n",
      "           2       0.74      0.81      0.78        32\n",
      "           3       0.63      0.57      0.60        21\n",
      "\n",
      "    accuracy                           0.79        92\n",
      "   macro avg       0.77      0.76      0.76        92\n",
      "weighted avg       0.79      0.79      0.79        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = svm, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.91      0.93      0.92       391\n",
      "         bcc       0.73      0.77      0.75       323\n",
      "         scc       0.61      0.52      0.56       207\n",
      "\n",
      "    accuracy                           0.78       921\n",
      "   macro avg       0.75      0.74      0.74       921\n",
      "weighted avg       0.78      0.78      0.78       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some important notes here, before running the Decision Tree Classifier, it's ok to not scale the features, as this model\n",
    "# does not work using euclidian distance, so it's fine as it is\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 20, random_state = 0, splitter = 'best')\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69,  7,  3],\n",
       "       [10, 41,  9],\n",
       "       [14, 18, 14]], dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6345057386153277\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = tree, X = X_train, y = y_train, cv = 10)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "{'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 20, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to Find the Best Hyperparameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "    {'criterion': ['entropy', 'gini'], 'splitter': ['best', 'random'], 'ccp_alpha': [0.0, 0.05, 0.1], 'max_depth': [10, 20, 30]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = tree, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(round(best_accuracy, 2))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.80      0.80        40\n",
      "           2       0.65      0.53      0.59        32\n",
      "           3       0.41      0.52      0.46        21\n",
      "\n",
      "    accuracy                           0.65        93\n",
      "   macro avg       0.62      0.62      0.61        93\n",
      "weighted avg       0.66      0.65      0.65        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.74      0.78        39\n",
      "           2       0.68      0.70      0.69        33\n",
      "           3       0.39      0.45      0.42        20\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.63      0.63      0.63        92\n",
      "weighted avg       0.68      0.66      0.67        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.79      0.75        39\n",
      "           2       0.71      0.67      0.69        33\n",
      "           3       0.47      0.40      0.43        20\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.63      0.62      0.62        92\n",
      "weighted avg       0.66      0.66      0.66        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.79      0.77        39\n",
      "           2       0.56      0.45      0.50        33\n",
      "           3       0.33      0.40      0.36        20\n",
      "\n",
      "    accuracy                           0.59        92\n",
      "   macro avg       0.55      0.55      0.55        92\n",
      "weighted avg       0.59      0.59      0.59        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.77      0.77        39\n",
      "           2       0.63      0.69      0.66        32\n",
      "           3       0.33      0.29      0.31        21\n",
      "\n",
      "    accuracy                           0.63        92\n",
      "   macro avg       0.58      0.58      0.58        92\n",
      "weighted avg       0.62      0.63      0.62        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.79      0.79        39\n",
      "           2       0.58      0.56      0.57        32\n",
      "           3       0.27      0.29      0.28        21\n",
      "\n",
      "    accuracy                           0.60        92\n",
      "   macro avg       0.55      0.55      0.55        92\n",
      "weighted avg       0.60      0.60      0.60        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.82      0.76        39\n",
      "           2       0.42      0.31      0.36        32\n",
      "           3       0.26      0.29      0.27        21\n",
      "\n",
      "    accuracy                           0.52        92\n",
      "   macro avg       0.46      0.47      0.46        92\n",
      "weighted avg       0.51      0.52      0.51        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.79      0.77        39\n",
      "           2       0.52      0.44      0.47        32\n",
      "           3       0.26      0.29      0.27        21\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.51      0.51      0.50        92\n",
      "weighted avg       0.55      0.55      0.55        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.77      0.86        39\n",
      "           2       0.71      0.75      0.73        32\n",
      "           3       0.52      0.67      0.58        21\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.73      0.73      0.72        92\n",
      "weighted avg       0.77      0.74      0.75        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.69      0.69        39\n",
      "           2       0.54      0.47      0.50        32\n",
      "           3       0.36      0.43      0.39        21\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.53      0.53      0.53        92\n",
      "weighted avg       0.56      0.55      0.56        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = tree, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.77      0.78      0.77       391\n",
      "         bcc       0.61      0.56      0.58       323\n",
      "         scc       0.36      0.40      0.38       207\n",
      "\n",
      "    accuracy                           0.62       921\n",
      "   macro avg       0.58      0.58      0.58       921\n",
      "weighted avg       0.62      0.62      0.62       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='entropy', max_depth=10, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 150, max_depth = 10, criterion = 'entropy', class_weight = 'balanced', random_state = 0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71,  7,  1],\n",
       "       [ 7, 47,  6],\n",
       "       [ 9, 20, 17]], dtype=int64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7337652721214365\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = forest, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n",
      "{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to Find the Best Hyperparameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "    {'criterion': ['entropy'], 'max_depth': [None, 10], 'n_estimators': [150, 200, 250], 'class_weight': [None, 'balanced', 'balanced_subsample']},\n",
    "    {'criterion': ['gini'], 'max_depth': [None, 10], 'n_estimators': [150, 200, 250], 'class_weight': [None, 'balanced', 'balanced_subsample']}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = forest, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(round(best_accuracy, 2))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.85        40\n",
      "           2       0.57      0.62      0.60        32\n",
      "           3       0.46      0.29      0.35        21\n",
      "\n",
      "    accuracy                           0.67        93\n",
      "   macro avg       0.61      0.60      0.60        93\n",
      "weighted avg       0.64      0.67      0.65        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.87      0.85        39\n",
      "           2       0.74      0.79      0.76        33\n",
      "           3       0.69      0.55      0.61        20\n",
      "\n",
      "    accuracy                           0.77        92\n",
      "   macro avg       0.75      0.74      0.74        92\n",
      "weighted avg       0.77      0.77      0.77        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.82      0.84        39\n",
      "           2       0.64      0.82      0.72        33\n",
      "           3       0.62      0.40      0.48        20\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.71      0.68      0.68        92\n",
      "weighted avg       0.73      0.73      0.72        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.90      0.86        39\n",
      "           2       0.68      0.70      0.69        33\n",
      "           3       0.56      0.45      0.50        20\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.69      0.68      0.68        92\n",
      "weighted avg       0.72      0.73      0.72        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.79      0.82        39\n",
      "           2       0.61      0.84      0.71        32\n",
      "           3       0.45      0.24      0.31        21\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.64      0.63      0.61        92\n",
      "weighted avg       0.67      0.68      0.66        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.82      0.84        39\n",
      "           2       0.69      0.62      0.66        32\n",
      "           3       0.50      0.62      0.55        21\n",
      "\n",
      "    accuracy                           0.71        92\n",
      "   macro avg       0.68      0.69      0.68        92\n",
      "weighted avg       0.72      0.71      0.71        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.90      0.88        39\n",
      "           2       0.55      0.66      0.60        32\n",
      "           3       0.38      0.24      0.29        21\n",
      "\n",
      "    accuracy                           0.66        92\n",
      "   macro avg       0.60      0.60      0.59        92\n",
      "weighted avg       0.64      0.66      0.65        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.87      0.84        39\n",
      "           2       0.72      0.81      0.76        32\n",
      "           3       0.57      0.38      0.46        21\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.70      0.69      0.69        92\n",
      "weighted avg       0.72      0.74      0.73        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.90      0.85        39\n",
      "           2       0.76      0.91      0.83        32\n",
      "           3       0.73      0.38      0.50        21\n",
      "\n",
      "    accuracy                           0.78        92\n",
      "   macro avg       0.77      0.73      0.73        92\n",
      "weighted avg       0.78      0.78      0.76        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.85      0.83        39\n",
      "           2       0.67      0.75      0.71        32\n",
      "           3       0.60      0.43      0.50        21\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.69      0.67      0.68        92\n",
      "weighted avg       0.71      0.72      0.71        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = forest, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.83      0.86      0.85       391\n",
      "         bcc       0.66      0.75      0.70       323\n",
      "         scc       0.55      0.40      0.46       207\n",
      "\n",
      "    accuracy                           0.72       921\n",
      "   macro avg       0.68      0.67      0.67       921\n",
      "weighted avg       0.71      0.72      0.71       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical input for predictor\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "Y = Y.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y_use = onehotencoder.fit_transform(Y).toarray()\n",
    "y_binary = onehotencoder.fit_transform(y_train).toarray()\n",
    "y_compare = onehotencoder.fit_transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736 samples, validate on 185 samples\n",
      "Epoch 1/100\n",
      "736/736 [==============================] - 13s 17ms/step - loss: 1.0643 - accuracy: 0.4606 - categorical_accuracy: 0.4606 - val_loss: 0.9918 - val_accuracy: 0.5027 - val_categorical_accuracy: 0.5027\n",
      "Epoch 2/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.9065 - accuracy: 0.5883 - categorical_accuracy: 0.5883 - val_loss: 0.9233 - val_accuracy: 0.5622 - val_categorical_accuracy: 0.5622\n",
      "Epoch 3/100\n",
      "736/736 [==============================] - 0s 287us/step - loss: 0.8365 - accuracy: 0.6182 - categorical_accuracy: 0.6182 - val_loss: 0.8787 - val_accuracy: 0.5514 - val_categorical_accuracy: 0.5514\n",
      "Epoch 4/100\n",
      "736/736 [==============================] - 0s 171us/step - loss: 0.7792 - accuracy: 0.6576 - categorical_accuracy: 0.6576 - val_loss: 0.8373 - val_accuracy: 0.5784 - val_categorical_accuracy: 0.5784\n",
      "Epoch 5/100\n",
      "736/736 [==============================] - 0s 101us/step - loss: 0.7330 - accuracy: 0.6807 - categorical_accuracy: 0.6807 - val_loss: 0.8098 - val_accuracy: 0.6216 - val_categorical_accuracy: 0.6216\n",
      "Epoch 6/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.6899 - accuracy: 0.7011 - categorical_accuracy: 0.7011 - val_loss: 0.7896 - val_accuracy: 0.6216 - val_categorical_accuracy: 0.6216\n",
      "Epoch 7/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.6546 - accuracy: 0.7174 - categorical_accuracy: 0.7174 - val_loss: 0.7511 - val_accuracy: 0.6703 - val_categorical_accuracy: 0.6703\n",
      "Epoch 8/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.6181 - accuracy: 0.7296 - categorical_accuracy: 0.7296 - val_loss: 0.7201 - val_accuracy: 0.7027 - val_categorical_accuracy: 0.7027\n",
      "Epoch 9/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.5765 - accuracy: 0.7527 - categorical_accuracy: 0.7527 - val_loss: 0.6886 - val_accuracy: 0.6973 - val_categorical_accuracy: 0.6973\n",
      "Epoch 10/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.5478 - accuracy: 0.7717 - categorical_accuracy: 0.7717 - val_loss: 0.6606 - val_accuracy: 0.7243 - val_categorical_accuracy: 0.7243\n",
      "Epoch 11/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.5205 - accuracy: 0.7785 - categorical_accuracy: 0.7785 - val_loss: 0.6434 - val_accuracy: 0.7297 - val_categorical_accuracy: 0.7297\n",
      "Epoch 12/100\n",
      "736/736 [==============================] - 0s 94us/step - loss: 0.4963 - accuracy: 0.7880 - categorical_accuracy: 0.7880 - val_loss: 0.6301 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 13/100\n",
      "736/736 [==============================] - 0s 105us/step - loss: 0.4773 - accuracy: 0.7948 - categorical_accuracy: 0.7948 - val_loss: 0.6099 - val_accuracy: 0.7405 - val_categorical_accuracy: 0.7405\n",
      "Epoch 14/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.4539 - accuracy: 0.8003 - categorical_accuracy: 0.8003 - val_loss: 0.6045 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 15/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.4474 - accuracy: 0.8003 - categorical_accuracy: 0.8003 - val_loss: 0.5954 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 16/100\n",
      "736/736 [==============================] - 0s 116us/step - loss: 0.4309 - accuracy: 0.8071 - categorical_accuracy: 0.8071 - val_loss: 0.5964 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 17/100\n",
      "736/736 [==============================] - 0s 315us/step - loss: 0.4231 - accuracy: 0.8139 - categorical_accuracy: 0.8139 - val_loss: 0.5834 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 18/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.4107 - accuracy: 0.8193 - categorical_accuracy: 0.8193 - val_loss: 0.5897 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 19/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.4040 - accuracy: 0.8220 - categorical_accuracy: 0.8220 - val_loss: 0.5915 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 20/100\n",
      "736/736 [==============================] - 0s 101us/step - loss: 0.3936 - accuracy: 0.8342 - categorical_accuracy: 0.8342 - val_loss: 0.6006 - val_accuracy: 0.7784 - val_categorical_accuracy: 0.7784\n",
      "Epoch 21/100\n",
      "736/736 [==============================] - 0s 94us/step - loss: 0.3885 - accuracy: 0.8179 - categorical_accuracy: 0.8179 - val_loss: 0.5910 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 22/100\n",
      "736/736 [==============================] - 0s 124us/step - loss: 0.3795 - accuracy: 0.8315 - categorical_accuracy: 0.8315 - val_loss: 0.5966 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 23/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.3716 - accuracy: 0.8342 - categorical_accuracy: 0.8342 - val_loss: 0.5875 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 24/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.3680 - accuracy: 0.8410 - categorical_accuracy: 0.8410 - val_loss: 0.5915 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 25/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.3582 - accuracy: 0.8438 - categorical_accuracy: 0.8438 - val_loss: 0.5739 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 26/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.3575 - accuracy: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5912 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 27/100\n",
      "736/736 [==============================] - 0s 118us/step - loss: 0.3499 - accuracy: 0.8519 - categorical_accuracy: 0.8519 - val_loss: 0.5826 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 28/100\n",
      "736/736 [==============================] - 0s 114us/step - loss: 0.3411 - accuracy: 0.8614 - categorical_accuracy: 0.8614 - val_loss: 0.5887 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 29/100\n",
      "736/736 [==============================] - 0s 107us/step - loss: 0.3368 - accuracy: 0.8573 - categorical_accuracy: 0.8573 - val_loss: 0.5767 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 30/100\n",
      "736/736 [==============================] - 0s 112us/step - loss: 0.3305 - accuracy: 0.8628 - categorical_accuracy: 0.8628 - val_loss: 0.5876 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 31/100\n",
      "736/736 [==============================] - 0s 120us/step - loss: 0.3258 - accuracy: 0.8696 - categorical_accuracy: 0.8696 - val_loss: 0.5871 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 32/100\n",
      "736/736 [==============================] - 0s 114us/step - loss: 0.3253 - accuracy: 0.8641 - categorical_accuracy: 0.8641 - val_loss: 0.5735 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 33/100\n",
      "736/736 [==============================] - 0s 106us/step - loss: 0.3157 - accuracy: 0.8764 - categorical_accuracy: 0.8764 - val_loss: 0.5835 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 34/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.3085 - accuracy: 0.8723 - categorical_accuracy: 0.8723 - val_loss: 0.6044 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 35/100\n",
      "736/736 [==============================] - 0s 118us/step - loss: 0.3069 - accuracy: 0.8777 - categorical_accuracy: 0.8777 - val_loss: 0.5973 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 36/100\n",
      "736/736 [==============================] - 0s 118us/step - loss: 0.3041 - accuracy: 0.8804 - categorical_accuracy: 0.8804 - val_loss: 0.6317 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 37/100\n",
      "736/736 [==============================] - 0s 119us/step - loss: 0.2968 - accuracy: 0.8791 - categorical_accuracy: 0.8791 - val_loss: 0.6131 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 38/100\n",
      "736/736 [==============================] - 0s 113us/step - loss: 0.3066 - accuracy: 0.8777 - categorical_accuracy: 0.8777 - val_loss: 0.6106 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 39/100\n",
      "736/736 [==============================] - 0s 112us/step - loss: 0.2946 - accuracy: 0.8764 - categorical_accuracy: 0.8764 - val_loss: 0.6009 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 40/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2961 - accuracy: 0.8764 - categorical_accuracy: 0.8764 - val_loss: 0.6080 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 41/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.2807 - accuracy: 0.8832 - categorical_accuracy: 0.8832 - val_loss: 0.6265 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 42/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.2763 - accuracy: 0.8845 - categorical_accuracy: 0.8845 - val_loss: 0.6510 - val_accuracy: 0.7405 - val_categorical_accuracy: 0.7405\n",
      "Epoch 43/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2747 - accuracy: 0.8967 - categorical_accuracy: 0.8967 - val_loss: 0.6336 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 44/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2753 - accuracy: 0.8981 - categorical_accuracy: 0.8981 - val_loss: 0.6213 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 45/100\n",
      "736/736 [==============================] - 0s 135us/step - loss: 0.2750 - accuracy: 0.8859 - categorical_accuracy: 0.8859 - val_loss: 0.6293 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 46/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.2678 - accuracy: 0.8954 - categorical_accuracy: 0.8954 - val_loss: 0.6460 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 47/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.2640 - accuracy: 0.8981 - categorical_accuracy: 0.8981 - val_loss: 0.6246 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 48/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.2573 - accuracy: 0.8927 - categorical_accuracy: 0.8927 - val_loss: 0.6711 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 49/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2527 - accuracy: 0.9049 - categorical_accuracy: 0.9049 - val_loss: 0.6396 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 50/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2506 - accuracy: 0.9049 - categorical_accuracy: 0.9049 - val_loss: 0.6324 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 51/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.2462 - accuracy: 0.9049 - categorical_accuracy: 0.9049 - val_loss: 0.6546 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 52/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2476 - accuracy: 0.9035 - categorical_accuracy: 0.9035 - val_loss: 0.6540 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 53/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.2373 - accuracy: 0.9185 - categorical_accuracy: 0.9185 - val_loss: 0.6434 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 54/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2404 - accuracy: 0.9049 - categorical_accuracy: 0.9049 - val_loss: 0.6468 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 55/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2458 - accuracy: 0.9062 - categorical_accuracy: 0.9062 - val_loss: 0.6832 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 56/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.2365 - accuracy: 0.9062 - categorical_accuracy: 0.9062 - val_loss: 0.6893 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 57/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.2273 - accuracy: 0.9103 - categorical_accuracy: 0.9103 - val_loss: 0.6796 - val_accuracy: 0.7784 - val_categorical_accuracy: 0.7784\n",
      "Epoch 58/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.2207 - accuracy: 0.9130 - categorical_accuracy: 0.9130 - val_loss: 0.6506 - val_accuracy: 0.7892 - val_categorical_accuracy: 0.7892\n",
      "Epoch 59/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.2138 - accuracy: 0.9226 - categorical_accuracy: 0.9226 - val_loss: 0.6715 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 60/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.2157 - accuracy: 0.9185 - categorical_accuracy: 0.9185 - val_loss: 0.6827 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 61/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2095 - accuracy: 0.9198 - categorical_accuracy: 0.9198 - val_loss: 0.7096 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 62/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.2097 - accuracy: 0.9239 - categorical_accuracy: 0.9239 - val_loss: 0.7025 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 63/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.2105 - accuracy: 0.9226 - categorical_accuracy: 0.9226 - val_loss: 0.6862 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 64/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1981 - accuracy: 0.9253 - categorical_accuracy: 0.9253 - val_loss: 0.6866 - val_accuracy: 0.7784 - val_categorical_accuracy: 0.7784\n",
      "Epoch 65/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.1986 - accuracy: 0.9253 - categorical_accuracy: 0.9253 - val_loss: 0.7194 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 66/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1922 - accuracy: 0.9280 - categorical_accuracy: 0.9280 - val_loss: 0.7059 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 67/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1925 - accuracy: 0.9361 - categorical_accuracy: 0.9361 - val_loss: 0.7373 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 68/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1930 - accuracy: 0.9266 - categorical_accuracy: 0.9266 - val_loss: 0.7040 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 69/100\n",
      "736/736 [==============================] - 0s 117us/step - loss: 0.1874 - accuracy: 0.9321 - categorical_accuracy: 0.9321 - val_loss: 0.7480 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n",
      "Epoch 70/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1902 - accuracy: 0.9348 - categorical_accuracy: 0.9348 - val_loss: 0.7592 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 71/100\n",
      "736/736 [==============================] - 0s 114us/step - loss: 0.1798 - accuracy: 0.9307 - categorical_accuracy: 0.9307 - val_loss: 0.7183 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 72/100\n",
      "736/736 [==============================] - 0s 114us/step - loss: 0.1804 - accuracy: 0.9334 - categorical_accuracy: 0.9334 - val_loss: 0.7319 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 73/100\n",
      "736/736 [==============================] - 0s 112us/step - loss: 0.1731 - accuracy: 0.9375 - categorical_accuracy: 0.9375 - val_loss: 0.7509 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 74/100\n",
      "736/736 [==============================] - 0s 118us/step - loss: 0.1711 - accuracy: 0.9429 - categorical_accuracy: 0.9429 - val_loss: 0.7487 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 75/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.1658 - accuracy: 0.9402 - categorical_accuracy: 0.9402 - val_loss: 0.7535 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 76/100\n",
      "736/736 [==============================] - 0s 103us/step - loss: 0.1670 - accuracy: 0.9389 - categorical_accuracy: 0.9389 - val_loss: 0.7596 - val_accuracy: 0.7676 - val_categorical_accuracy: 0.7676\n",
      "Epoch 77/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.1702 - accuracy: 0.9375 - categorical_accuracy: 0.9375 - val_loss: 0.8027 - val_accuracy: 0.7730 - val_categorical_accuracy: 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.1607 - accuracy: 0.9389 - categorical_accuracy: 0.9389 - val_loss: 0.8087 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 79/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.1545 - accuracy: 0.9416 - categorical_accuracy: 0.9416 - val_loss: 0.8241 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 80/100\n",
      "736/736 [==============================] - 0s 104us/step - loss: 0.1523 - accuracy: 0.9443 - categorical_accuracy: 0.9443 - val_loss: 0.8386 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 81/100\n",
      "736/736 [==============================] - 0s 103us/step - loss: 0.1476 - accuracy: 0.9470 - categorical_accuracy: 0.9470 - val_loss: 0.8629 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 82/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.1425 - accuracy: 0.9538 - categorical_accuracy: 0.9538 - val_loss: 0.8291 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 83/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.1518 - accuracy: 0.9402 - categorical_accuracy: 0.9402 - val_loss: 0.8222 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 84/100\n",
      "736/736 [==============================] - 0s 99us/step - loss: 0.1438 - accuracy: 0.9497 - categorical_accuracy: 0.9497 - val_loss: 0.8508 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 85/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.1410 - accuracy: 0.9457 - categorical_accuracy: 0.9457 - val_loss: 0.8436 - val_accuracy: 0.7784 - val_categorical_accuracy: 0.7784\n",
      "Epoch 86/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.1311 - accuracy: 0.9497 - categorical_accuracy: 0.9497 - val_loss: 0.8830 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 87/100\n",
      "736/736 [==============================] - 0s 108us/step - loss: 0.1365 - accuracy: 0.9484 - categorical_accuracy: 0.9484 - val_loss: 0.8963 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 88/100\n",
      "736/736 [==============================] - 0s 120us/step - loss: 0.1332 - accuracy: 0.9579 - categorical_accuracy: 0.9579 - val_loss: 0.9278 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 89/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.1272 - accuracy: 0.9579 - categorical_accuracy: 0.9579 - val_loss: 0.8715 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 90/100\n",
      "736/736 [==============================] - 0s 95us/step - loss: 0.1270 - accuracy: 0.9606 - categorical_accuracy: 0.9606 - val_loss: 0.9219 - val_accuracy: 0.7297 - val_categorical_accuracy: 0.7297\n",
      "Epoch 91/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1435 - accuracy: 0.9470 - categorical_accuracy: 0.9470 - val_loss: 0.8575 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 92/100\n",
      "736/736 [==============================] - 0s 434us/step - loss: 0.1212 - accuracy: 0.9565 - categorical_accuracy: 0.9565 - val_loss: 0.9393 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 93/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.1176 - accuracy: 0.9552 - categorical_accuracy: 0.9552 - val_loss: 0.8997 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 94/100\n",
      "736/736 [==============================] - 0s 101us/step - loss: 0.1127 - accuracy: 0.9592 - categorical_accuracy: 0.9592 - val_loss: 0.9509 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 95/100\n",
      "736/736 [==============================] - 0s 96us/step - loss: 0.1113 - accuracy: 0.9688 - categorical_accuracy: 0.9688 - val_loss: 0.9228 - val_accuracy: 0.7514 - val_categorical_accuracy: 0.7514\n",
      "Epoch 96/100\n",
      "736/736 [==============================] - 0s 97us/step - loss: 0.1117 - accuracy: 0.9674 - categorical_accuracy: 0.9674 - val_loss: 1.0129 - val_accuracy: 0.7405 - val_categorical_accuracy: 0.7405\n",
      "Epoch 97/100\n",
      "736/736 [==============================] - 0s 98us/step - loss: 0.1143 - accuracy: 0.9647 - categorical_accuracy: 0.9647 - val_loss: 0.9305 - val_accuracy: 0.7568 - val_categorical_accuracy: 0.7568\n",
      "Epoch 98/100\n",
      "736/736 [==============================] - 0s 124us/step - loss: 0.1090 - accuracy: 0.9688 - categorical_accuracy: 0.9688 - val_loss: 0.9487 - val_accuracy: 0.7459 - val_categorical_accuracy: 0.7459\n",
      "Epoch 99/100\n",
      "736/736 [==============================] - 0s 103us/step - loss: 0.1031 - accuracy: 0.9688 - categorical_accuracy: 0.9688 - val_loss: 1.0057 - val_accuracy: 0.7622 - val_categorical_accuracy: 0.7622\n",
      "Epoch 100/100\n",
      "736/736 [==============================] - 0s 100us/step - loss: 0.1057 - accuracy: 0.9633 - categorical_accuracy: 0.9633 - val_loss: 0.9809 - val_accuracy: 0.7297 - val_categorical_accuracy: 0.7297\n"
     ]
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "ann = Sequential()\n",
    "\n",
    "# Adding the input & hidden layer\n",
    "ann.add(Dense(16, activation = 'relu', input_shape = (61,)))\n",
    "ann.add(Dense(16, activation = 'relu'))\n",
    "ann.add(Dense(3, activation = 'softmax'))\n",
    "ann.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2),\n",
    "            ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', save_best_only = True)]\n",
    "\n",
    "history = ann.fit(X_train, y_binary, batch_size = 10, epochs = 100, validation_data = (X_test, y_compare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'val_categorical_accuracy', 'loss', 'accuracy', 'categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create threshold for multi-predictor\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76,  2,  1],\n",
       "       [ 6, 39, 15],\n",
       "       [ 7, 20, 19]], dtype=int64)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_compare.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.53      0.56      0.55        32\n",
      "           2       0.33      0.24      0.28        21\n",
      "\n",
      "    accuracy                           0.66        93\n",
      "   macro avg       0.58      0.58      0.58        93\n",
      "weighted avg       0.63      0.66      0.64        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        39\n",
      "           1       0.75      0.73      0.74        33\n",
      "           2       0.48      0.50      0.49        20\n",
      "\n",
      "    accuracy                           0.75        92\n",
      "   macro avg       0.71      0.71      0.71        92\n",
      "weighted avg       0.75      0.75      0.75        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91        39\n",
      "           1       0.77      0.70      0.73        33\n",
      "           2       0.60      0.60      0.60        20\n",
      "\n",
      "    accuracy                           0.78        92\n",
      "   macro avg       0.75      0.75      0.75        92\n",
      "weighted avg       0.78      0.78      0.78        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88        39\n",
      "           1       0.66      0.70      0.68        33\n",
      "           2       0.47      0.45      0.46        20\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.68      0.67      0.67        92\n",
      "weighted avg       0.72      0.72      0.72        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85        39\n",
      "           1       0.65      0.81      0.72        32\n",
      "           2       0.56      0.48      0.51        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.71      0.69      0.69        92\n",
      "weighted avg       0.74      0.73      0.73        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        39\n",
      "           1       0.80      0.75      0.77        32\n",
      "           2       0.52      0.57      0.55        21\n",
      "\n",
      "    accuracy                           0.77        92\n",
      "   macro avg       0.74      0.74      0.74        92\n",
      "weighted avg       0.78      0.77      0.77        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88        39\n",
      "           1       0.61      0.69      0.65        32\n",
      "           2       0.40      0.29      0.33        21\n",
      "\n",
      "    accuracy                           0.68        92\n",
      "   macro avg       0.62      0.62      0.62        92\n",
      "weighted avg       0.67      0.68      0.67        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        39\n",
      "           1       0.69      0.78      0.74        32\n",
      "           2       0.59      0.48      0.53        21\n",
      "\n",
      "    accuracy                           0.78        92\n",
      "   macro avg       0.74      0.74      0.74        92\n",
      "weighted avg       0.78      0.78      0.78        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91        39\n",
      "           1       0.72      0.88      0.79        32\n",
      "           2       0.65      0.52      0.58        21\n",
      "\n",
      "    accuracy                           0.79        92\n",
      "   macro avg       0.77      0.76      0.76        92\n",
      "weighted avg       0.80      0.79      0.79        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        39\n",
      "           1       0.69      0.75      0.72        32\n",
      "           2       0.47      0.38      0.42        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.68      0.68      0.67        92\n",
      "weighted avg       0.72      0.73      0.72        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "#, scoring=make_scorer(classification_report_with_accuracy_score)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Build the Model\n",
    "def buildmodel():\n",
    "    ann = Sequential()\n",
    "    ann.add(Dense(16, activation = 'relu', input_shape = (61,)))\n",
    "    ann.add(Dense(16, activation = 'relu'))\n",
    "    ann.add(Dense(3, activation = 'softmax'))\n",
    "    ann.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\n",
    "    return(ann)\n",
    "\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_use, y_pred):\n",
    "    originalclass.extend(y_use)\n",
    "    predictedclass.extend(y_pred.argmax(axis=1))\n",
    "    print(classification_report(y_use, y_pred.argmax(axis=1)))\n",
    "    return accuracy_score(y_use, y_pred.argmax(axis=1)) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "ann = KerasRegressor(build_fn=buildmodel, epochs = 100, batch_size = 10, verbose = 0)\n",
    "nested_score = cross_val_score(estimator = ann, X = X, y = y_use.argmax(1), cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.90      0.90      0.90       391\n",
      "         bcc       0.68      0.73      0.71       323\n",
      "         scc       0.51      0.45      0.48       207\n",
      "\n",
      "    accuracy                           0.74       921\n",
      "   macro avg       0.70      0.69      0.69       921\n",
      "weighted avg       0.73      0.74      0.74       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              normalize_type='forest', nthread=None, objective='multi:softprob',\n",
       "              random_state=0, rate_drop=0.1, reg_alpha=0, reg_lambda=1,\n",
       "              sample_type='uniform', scale_pos_weight=1, seed=None, silent=None,\n",
       "              skip_drop=0, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the XGB Library and fit it to the data\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(objective = 'multi:softmax', booster = 'dart', n_estimators = 100, normalize_type = 'forest', rate_drop = 0.1, sample_type = 'uniform', skip_drop = 0)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75,  3,  1],\n",
       "       [ 6, 48,  6],\n",
       "       [ 8, 24, 14]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test set result\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm2 = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310255460940392\n"
     ]
    }
   ],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = xgb, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "{'booster': 'dart', 'n_estimators': 100, 'normalize_type': 'forest', 'objective': 'multi:softmax', 'rate_drop': 0.1, 'sample_type': 'uniform', 'skip_drop': 0}\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to Find the Best Hyperparameter\n",
    "# {'objective': ['multi:softmax'], 'n_estimators': [25, 50, 100], 'booster': ['gbtree'], 'eta': [0.001, 0.002, 0.004], 'max_depth':[3], 'gamma': [0], 'subsample': [1], 'sampling_method': ['uniform', 'gradient_based']},\n",
    "# {'objective': ['multi:softmax'], 'n_estimators': [109], 'booster': ['dart'], 'sample_type': ['uniform', 'weighted'], 'normalize_type': ['tree', 'forest'], 'rate_drop': [0, 0.1, 0.2], 'skip_drop': [0, 0.1, 0.2]}\n",
    "# {'objective': ['multi:softmax'], 'n_estimators': [10, 25, 50], 'booster': ['gblinear'], 'feature_selector': ['greedy', 'cyclic', 'random', 'shuffle', 'thrifty'], 'updater': ['shotgun', 'coord_descent'], 'lambda': [0, 0.5, 1], 'alpha': [0, 0.5, 1]}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "    {'objective': ['multi:softmax'], 'n_estimators': [25, 50, 100], 'booster': ['gbtree'], 'eta': [0.001, 0.002, 0.004], 'max_depth':[3], 'gamma': [0], 'subsample': [1], 'sampling_method': ['uniform', 'gradient_based']},\n",
    "    {'objective': ['multi:softmax'], 'n_estimators': [25, 50, 100], 'booster': ['dart'], 'sample_type': ['uniform', 'weighted'], 'normalize_type': ['tree', 'forest'], 'rate_drop': [0, 0.1, 0.2], 'skip_drop': [0, 0.1, 0.2]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = xgb, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(round(best_accuracy, 2))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.93      0.88        40\n",
      "           2       0.57      0.62      0.60        32\n",
      "           3       0.36      0.24      0.29        21\n",
      "\n",
      "    accuracy                           0.67        93\n",
      "   macro avg       0.59      0.60      0.59        93\n",
      "weighted avg       0.64      0.67      0.65        93\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.87      0.87        39\n",
      "           2       0.81      0.88      0.84        33\n",
      "           3       0.71      0.60      0.65        20\n",
      "\n",
      "    accuracy                           0.82        92\n",
      "   macro avg       0.79      0.78      0.79        92\n",
      "weighted avg       0.81      0.82      0.81        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.90      0.86        39\n",
      "           2       0.67      0.79      0.72        33\n",
      "           3       0.55      0.30      0.39        20\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.68      0.66      0.66        92\n",
      "weighted avg       0.71      0.73      0.71        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.92      0.87        39\n",
      "           2       0.66      0.70      0.68        33\n",
      "           3       0.46      0.30      0.36        20\n",
      "\n",
      "    accuracy                           0.71        92\n",
      "   macro avg       0.65      0.64      0.64        92\n",
      "weighted avg       0.68      0.71      0.69        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.82      0.83        39\n",
      "           2       0.63      0.84      0.72        32\n",
      "           3       0.27      0.14      0.19        21\n",
      "\n",
      "    accuracy                           0.67        92\n",
      "   macro avg       0.58      0.60      0.58        92\n",
      "weighted avg       0.64      0.67      0.65        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.87      0.86        39\n",
      "           2       0.71      0.78      0.75        32\n",
      "           3       0.47      0.38      0.42        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.68      0.68      0.68        92\n",
      "weighted avg       0.72      0.73      0.72        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.97      0.90        39\n",
      "           2       0.69      0.75      0.72        32\n",
      "           3       0.67      0.38      0.48        21\n",
      "\n",
      "    accuracy                           0.76        92\n",
      "   macro avg       0.73      0.70      0.70        92\n",
      "weighted avg       0.75      0.76      0.74        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91        39\n",
      "           2       0.69      0.69      0.69        32\n",
      "           3       0.45      0.48      0.47        21\n",
      "\n",
      "    accuracy                           0.73        92\n",
      "   macro avg       0.69      0.69      0.69        92\n",
      "weighted avg       0.73      0.73      0.73        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.95      0.92        39\n",
      "           2       0.75      0.94      0.83        32\n",
      "           3       0.82      0.43      0.56        21\n",
      "\n",
      "    accuracy                           0.83        92\n",
      "   macro avg       0.82      0.77      0.77        92\n",
      "weighted avg       0.83      0.83      0.81        92\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.87      0.86        39\n",
      "           2       0.68      0.81      0.74        32\n",
      "           3       0.64      0.43      0.51        21\n",
      "\n",
      "    accuracy                           0.75        92\n",
      "   macro avg       0.73      0.70      0.71        92\n",
      "weighted avg       0.75      0.75      0.74        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy and other metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "target_names = ['melanoma', 'bcc', 'scc']\n",
    "\n",
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_test, y_pred):\n",
    "    originalclass.extend(y_test)\n",
    "    predictedclass.extend(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy_score(y_test, y_pred) # return accuracy score\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(estimator = xgb, X = X, y = Y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    melanoma       0.86      0.90      0.88       391\n",
      "         bcc       0.68      0.78      0.73       323\n",
      "         scc       0.54      0.37      0.44       207\n",
      "\n",
      "    accuracy                           0.74       921\n",
      "   macro avg       0.69      0.68      0.68       921\n",
      "weighted avg       0.72      0.74      0.73       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
